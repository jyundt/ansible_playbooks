---
# Oh yeah, this couldn't definitely be DRY-ed out from master.
- name: Open kubernetes worker firewalld ports
  firewalld:
    port: "{{ item }}"
    state: enabled
    permanent: true
    immediate: true
  with_items: "{{ k8s_worker_ports }}"

# This task exists for an edge case where a worker may have previously been
# defined as a master
- name: Close kubernetes master firewalld ports
  firewalld:
    port: "{{ item }}"
    state: disabled
    permanent: true
    immediate: true
  with_items: "{{ k8s_master_ports | difference(k8s_worker_ports) }}"
  when: "'master' not in group_names"

# We need to run this on a master
- name: Gather list of nodes in cluster
  command: kubectl get nodes --kubeconfig /etc/kubernetes/admin.conf --no-headers
  delegate_to: "{{ groups['configured_master'] | first }}"
  changed_when: false
  failed_when: false
  register: kubectl_output

- name: Derive list of master nodes according to kubectl
  set_fact:
    kubectl_worker: "{{ kubectl_output.stdout | regex_findall('^(\\w+)\\s+\\w+\\s+(?:worker|master)', '\\1') }}"
  when: kubectl_output.rc == 0

# add_host is similar to run_once, it only executes once on one host.
# Because of that, we'll loop through hosts in the worker group
- name: Build list of configured worker nodes
  add_host:
    name: "{{ item }}"
    groups: configured_worker
  when: item in hostvars[item]['kubectl_worker'] | default([])
  with_items: "{{ groups['worker'] }}"
  changed_when: False

# Block of tasks to skip if all worker nodes are configured
- name: Join unconfigured worker nodes to cluster
  block:
  - name: Generate a join token
    command: kubeadm token create --print-join-command
    register: kubeadm_join_token_output
    run_once: true
    delegate_to: "{{ groups['configured_master'] | first }}"

  - name: Join unconfigured worker to cluster
    command: "{{ kubeadm_join_token_output.stdout }}"
    when: "'configured_worker' not in group_names"

  - name: Apply worker label
    command: " kubectl --kubeconfig /etc/kubernetes/admin.conf label node {{ inventory_hostname }} node-role.kubernetes.io/worker=worker"
    delegate_to: "{{ groups['configured_master'] | first }}"
    when: "'configured_worker' not in group_names"

  # Cleanup taints on hybrid master/worker nodes
  - name: Check taints for hybrid master/worker
    command: "kubectl describe node {{ inventory_hostname }} --kubeconfig /etc/kubernetes/admin.conf"
    delegate_to: "{{ groups['configured_master'] | first }}"
    register: kubectl_output
    changed_when: False
    when:
      - "'worker' in group_names"
      - "'master' in group_names"

  - name: Untaint hybrid master/worker nodes
    command: "kubectl taint node {{ inventory_hostname }} node-role.kubernetes.io/master- --kubeconfig /etc/kubernetes/admin.conf"
    delegate_to: "{{ groups['configured_master'] | first }}"
    when:
      - "'worker' in group_names"
      - "'master' in group_names"
      - kubectl_output.stdout | regex_search('node-role.kubernetes.io/master:NoSchedule')

  when: groups['configured_worker'] | default([]) != groups['worker'] | difference(groups['master'])
